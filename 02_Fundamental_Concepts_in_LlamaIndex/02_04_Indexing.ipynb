{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-cohere--0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COHERE_API_KEY=ExDWFBWleIsWuYma4FhYUsEIC3qTuINe49vhibsK'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Indexing\n",
    "\n",
    "An `Index` is a data structure that allows for the quick retrieval of relevant context for a user query. \n",
    "\n",
    "It is the core foundation for retrieval-augmented generation (RAG) use-cases. Indexes are built from `Documents` and are used to build Retrievers, Query Engines and Chat Engines. All of which enable question & answer and chat over your data.\n",
    "\n",
    "- üìÇ After loading your data, you're ready to construct an `Index`.\n",
    "\n",
    "- üåê **Vector Store Index:** The most common Index type. It segments your `Documents` into `Nodes` and generates vector embeddings for each node's text, prepping them for LLM queries.\n",
    "\n",
    "- üîÑ **Vector Store Index Process:** Parse raw texts into document objects, split document objects into chunks/nodes, then convert all your nodes into embeddings and store them in a vector database.\n",
    "\n",
    "### ‚öôÔ∏è Embedding Text\n",
    "\n",
    "First, let's see what an embedding is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/conda/envs/lil_llama_index/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "embed_v3 = CohereEmbedding(model_name=\"embed-english-v3.0\")\n",
    "\n",
    "embed_v3_light = CohereEmbedding(model_name=\"embed-english-light-v3.0\")\n",
    "\n",
    "embed_v2 = CohereEmbedding(model_name=\"embed-english-v2.0\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also use local embedding models, by using an embedding model from Hugging Face. Check the [MTEB Leaderboard for what's hot](huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "```python\n",
    "\n",
    "pip install llama-index-embeddings-huggingface\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "hf_embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "```\n",
    "\n",
    "#### If you're running locally and on a CPU, though, you may want to use `FastEmbed`. These models are lightweight, quantized, and optimized for CPU. Here are the [supported models](https://qdrant.github.io/fastembed/examples/Supported_Models/)\n",
    "\n",
    "This is how you can instantiate a `FastEmbed` model:\n",
    "\n",
    "```python\n",
    "pip install llama-index-embeddings-fastembed\n",
    "\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-large-en-v1.5-quantized\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"A\"\n",
    "\n",
    "string_2 = \"This is a complete sentence.\"\n",
    "\n",
    "string_3 = \"\"\"In the pursuit of a life well-lived, one must recognize the transient nature of the \n",
    "material world and the enduring value of virtue. The Sikh Gurus taught us that the Divine Light \n",
    "resides within all, and thus, we are united in our essence beyond the superficial distinctions of \n",
    "caste, creed, or status. Similarly, the Stoics emphasized the cultivation of inner virtues such as courage, \n",
    "temperance, and wisdom, understanding that true freedom lies in mastery over one's own perceptions and actions. \n",
    "As we navigate the vicissitudes of life, let us remember that our choices are our own, and in choosing virtue, \n",
    "we align ourselves with the cosmic order and the teachings of the Gurus. It is through selfless service, \n",
    "compassion, and the relentless pursuit of truth that we may attain a state of inner peace and contribute \n",
    "to the harmony of the world, embodying the principles of both Sikhism and Stoicism in our daily lives\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "status_code: 401, body: {'message': 'invalid api token'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example_embedding \u001b[38;5;241m=\u001b[39m \u001b[43membed_v3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/base/embeddings/base.py:257\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    249\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    250\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    251\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    255\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()}\n\u001b[1;32m    256\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 257\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    260\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    261\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: [text],\n\u001b[1;32m    262\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: [text_embedding],\n\u001b[1;32m    263\u001b[0m         }\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    265\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    266\u001b[0m     EmbeddingEndEvent(\n\u001b[1;32m    267\u001b[0m         chunks\u001b[38;5;241m=\u001b[39m[text],\n\u001b[1;32m    268\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39m[text_embedding],\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/embeddings/cohere/base.py:241\u001b[0m, in \u001b[0;36mCohereEmbedding._get_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get text embedding.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/embeddings/cohere/base.py:192\u001b[0m, in \u001b[0;36mCohereEmbedding._embed\u001b[0;34m(self, texts, input_type)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed sentences using Cohere.\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01min\u001b[39;00m V3_MODELS:\n\u001b[0;32m--> 192\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcohere_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membeddings\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcohere_client\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    201\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    202\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    203\u001b[0m         embedding_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_type],\n\u001b[1;32m    204\u001b[0m         truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate,\n\u001b[1;32m    205\u001b[0m     )\u001b[38;5;241m.\u001b[39membeddings\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/cohere/client.py:137\u001b[0m, in \u001b[0;36mClient.embed\u001b[0;34m(self, texts, model, input_type, embedding_types, truncate, request_options, batching)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    127\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [texts[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), embed_batch_size)]\n\u001b[0;32m--> 137\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    138\u001b[0m     response\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    142\u001b[0m             texts\u001b[38;5;241m=\u001b[39mtext_batch,\n\u001b[1;32m    143\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    144\u001b[0m             input_type\u001b[38;5;241m=\u001b[39minput_type,\n\u001b[1;32m    145\u001b[0m             embedding_types\u001b[38;5;241m=\u001b[39membedding_types,\n\u001b[1;32m    146\u001b[0m             truncate\u001b[38;5;241m=\u001b[39mtruncate,\n\u001b[1;32m    147\u001b[0m             request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    148\u001b[0m         ),\n\u001b[1;32m    149\u001b[0m         texts_batches,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m ]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/cohere/client.py:137\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    127\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [texts[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), embed_batch_size)]\n\u001b[0;32m--> 137\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    138\u001b[0m     response\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    142\u001b[0m             texts\u001b[38;5;241m=\u001b[39mtext_batch,\n\u001b[1;32m    143\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    144\u001b[0m             input_type\u001b[38;5;241m=\u001b[39minput_type,\n\u001b[1;32m    145\u001b[0m             embedding_types\u001b[38;5;241m=\u001b[39membedding_types,\n\u001b[1;32m    146\u001b[0m             truncate\u001b[38;5;241m=\u001b[39mtruncate,\n\u001b[1;32m    147\u001b[0m             request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    148\u001b[0m         ),\n\u001b[1;32m    149\u001b[0m         texts_batches,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m ]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/cohere/client.py:140\u001b[0m, in \u001b[0;36mClient.embed.<locals>.<lambda>\u001b[0;34m(text_batch)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    127\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [texts[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), embed_batch_size)]\n\u001b[1;32m    137\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    138\u001b[0m     response\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: \u001b[43mBaseCohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43membedding_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    149\u001b[0m         texts_batches,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m ]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/cohere/base_client.py:1235\u001b[0m, in \u001b[0;36mBaseCohere.embed\u001b[0;34m(self, texts, model, input_type, embedding_types, truncate, request_options)\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m-> 1235\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response_json)\n",
      "\u001b[0;31mApiError\u001b[0m: status_code: 401, body: {'message': 'invalid api token'}"
     ]
    }
   ],
   "source": [
    "example_embedding = embed_v3.get_text_embedding(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mexample_embedding\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "len(example_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_dimensions(embed_model, list_of_strings):\n",
    "    embeddings = embed_model.get_text_embedding_batch(list_of_strings)   \n",
    "    embed_lens = []\n",
    "    for embedding in embeddings:\n",
    "        embed_lens.append(len(embedding))\n",
    "    return embed_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_dimensions(embed_v3, [string, string_2, string_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_dimensions(embed_v3_light, [string, string_2, string_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_dimensions(embed_v2, [string, string_2, string_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_v3.similarity(\n",
    "    embed_v3.get_text_embedding(\"\"\"In embracing both the wisdom of the Sikh Gurus and the Stoic philosophers, \n",
    "                              we find a path to tranquility by accepting what is beyond our control and focusing \n",
    "                              our efforts on living virtuously and with purpose.\"\"\"), \n",
    "    embed_v3.get_text_embedding(string_2),\n",
    "    mode=\"cosine\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Index\n",
    "\n",
    "First, let's get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def load_text_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches and returns the text content from the specified URL.\n",
    "\n",
    "    Parameters:\n",
    "    - url: The URL of the text file to fetch.\n",
    "\n",
    "    Returns:\n",
    "    - The text content of the file if the request is successful; otherwise, an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # This will raise an HTTPError if the response was an error\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Failed to load content from {url}. Error: {e}\"\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/10763/10763.txt\"\n",
    "\n",
    "text_content = load_text_from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚è≥ Generating embeddings can be time-consuming, especially with large volumes of text, due to numerous API calls required. \n",
    "\n",
    "Now, create an index by passing a **list of Documents**. To save time, and cost, we will only use 10,000 characters of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "full_document = Document(text=text_content)\n",
    "\n",
    "partial_document = Document(text=text_content[50000:60000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VectorStoreIndex` in LlamaIndex can be created in two ways: `from_documents` and `from_vector_store`.\n",
    "\n",
    "- `from_documents`: when you have a set of documents that you want to index. This method takes these documents, computes their embeddings, and stores them in the vector store. \n",
    "\n",
    "- `from_vector_store`: when you already have computed embeddings that are stored in an external vector store (like Qdrant). This method connects to the external vector store and uses the pre-computed embeddings for the index. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    # remember, you must pass a list of documents!\n",
    "    [partial_document], \n",
    "    embed_model=embed_v3,\n",
    "    show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you can also build an index over a **list of `Node` objects**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# instantiate a node parser\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=16,\n",
    "    paragraph_separator=\"\\n\\n\\n\\n\",\n",
    ")\n",
    "\n",
    "# pass a list of documents to the node paraser\n",
    "nodes = splitter.get_nodes_from_documents([partial_document])\n",
    "\n",
    "# create the index from the nodes\n",
    "index_from_nodes = VectorStoreIndex(\n",
    "    nodes,\n",
    "    embed_model=embed_v3,\n",
    "    show_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build on this pattern in the next lesson, where we'll store and persist our index for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
