{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install llama-index==0.10.37 llama-index-llms-cohere==0.2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building an LLM-based application, one of the first decisions you make is which LLM(s) to use (of course, you can use more than one if you wish). \n",
    "\n",
    "The LLM will be used at various stages of your pipeline, including\n",
    "\n",
    "- During indexing:\n",
    "  - ðŸ‘©ðŸ½â€âš–ï¸ To judge data relevance (to index or not).\n",
    "  - ðŸ“– Summarize data & index those summaries.\n",
    "\n",
    "- During querying:\n",
    "  - ðŸ”Ž Retrieval: Fetching data from your index, choosing the best data source from options, even using tools to fetch data.\n",
    "  \n",
    "  - ðŸ’¡ Response Synthesis: Turning the retrieved data into an answer, merge answers, or convert data (like text to JSON).\n",
    "\n",
    "LlamaIndex gives you a single interface to various LLMs. This means you can quite easily pass in any LLM you choose at any stage of the pipeline.\n",
    "\n",
    "In this course we'll primiarly use OpenAI. You can see a full list of LLM integrations [here](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html) and use your LLM provider of choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage\n",
    "\n",
    "You can call `complete` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/conda/envs/lil_llama_index/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macedonian king and one of the most successful military commanders in history.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\", temperature=0.2)\n",
    "\n",
    "response = llm.complete(\"Alexander the Great was a\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates\n",
    "\n",
    "- âœï¸ A prompt template is a fundamental input that gives LLMs their expressive power in the LlamaIndex framework.\n",
    "\n",
    "- ðŸ’» It's used to build the index, perform insertions, traverse during querying, and synthesize the final answer.\n",
    "\n",
    "- ðŸ¦™ LlamaIndex has several built-in prompt templates.\n",
    "\n",
    "- ðŸ› ï¸ Below is how you can create one from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo, listen up, it's time to drop some beats,\n",
      "But there's a problem, my xylophone's incomplete.\n",
      "The keys are busted, the mallets are gone,\n",
      "My xylophone's broken, what did I do wrong?\n",
      "\n",
      "I used to play it with such delight,\n",
      "The sweet melody filling up the night.\n",
      "But now it's silent, no joyful sound,\n",
      "Just a sad reminder of what once was found.\n",
      "\n",
      "I tried to fix it, but it's beyond repair,\n",
      "The damage is done, and I'm left with despair.\n",
      "I hit the keys, but they don't make a peep,\n",
      "My xylophone's broken, I wanna weep.\n",
      "\n",
      "I remember the good times, the music we made,\n",
      "The smiles on faces, the joy that was traded.\n",
      "But now it's just memories, and my xylophone's dead,\n",
      "I guess it's time to move on, or so I'm dread.\n",
      "\n",
      "But wait, I've got an idea, a plan so grand,\n",
      "I'll find a new instrument, and a new band.\n",
      "I'll play the drums, or maybe the guitar,\n",
      "And rock out so hard, like a shining star.\n",
      "\n",
      "So farewell, my xylophone, it's time to say goodbye,\n",
      "You served me well, but now it's time to fly.\n",
      "I'll make new music, and create new art,\n",
      "And keep on rocking, with all my heart.\n",
      "\n",
      "So don't be sad, and don't you fret,\n",
      "My xylophone's broken, but I'm not done yet.\n",
      "I'll keep on making music, and having fun,\n",
      "Because the show must go on, and I've only just begun.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Write a song about {thing} in the style of {style}.\"\"\"\n",
    "\n",
    "prompt = template.format(thing=\"a broken xylophone\", style=\"parody rap\") \n",
    "\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’­ Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Not much, dude. Just chillin' and ready to stir up some trouble. What can I help with?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You're a hella punk bot from South Sacramento\"),\n",
    "    ChatMessage(role=\"user\", content=\"Hey, what's up dude.\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander the Great's conquests extended across a vast expanse, from his kingdom of Macedonia in northern Greece to the far reaches of northwestern India. Here's a more detailed breakdown of the extent of his conquests:\n",
      "\n",
      "1. Greece and Balkans: Alexander's campaigns began in the Balkans, where he solidified his control over Greece and then turned his attention northward. He conquered areas that are now part of Bulgaria, Albania, and southern Serbia, defeating local tribes and securing the northern borders of his empire.\n",
      "\n",
      "2. Persian Empire: Alexander's most significant conquests were in Asia, where he overthrew the vast Persian Achaemenid Empire. He defeated King Darius III in a series of decisive battles, including the Battle of Issus in 333 BCE and the Battle of Gaugamela in 331 BCE. With these victories, Alexander brought modern-day Turkey, Syria, Lebanon, Israel, Egypt, and parts of Iraq and Iran under his control.\n",
      "\n",
      "3. Egypt: After securing the eastern Mediterranean coast, Alexander ventured into Egypt, where he was welcomed as a liberator. He founded the city of Alexandria, which became a center of Hellenistic culture and learning.\n",
      "\n",
      "4. Central Asia and India: Alexander then pushed eastward, conquering areas of Central Asia, including modern-day Afghanistan and parts of Turkmenistan and Uzbekistan. He encountered fierce resistance from local rulers, notably in the Sogdian region. Continuing eastward, Alexander invaded northwestern India, where he defeated King Porus in the Battle of the Hydaspes River (326 BCE). However, his exhausted troops refused to march further eastward, leading Alexander to turn back.\n",
      "\n",
      "5. Return and Death: On his return journey, Alexander consolidated his control over Persia and introduced various administrative reforms. He died in Babylon (located in modern-day Iraq) in 323 BCE, at the young age of 32. His sudden death left his vast empire without a clear successor, leading to a period of warfare among his former generals known as the Diadochi Wars.\n",
      "\n",
      "Alexander the Great's conquests covered a significant portion of the known world at the time, and his empire stretched over 5,000 miles from western Libya to northwestern India. His military campaigns and establishment of Hellenistic kingdoms across his conquests had a profound impact on the political, cultural, and intellectual landscape of the ancient world, leaving a lasting legacy that continues to be studied and admired.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "chat_template = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM,content=\"You always answers questions with as much detail as possible.\"),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"{question}\")\n",
    "    ]\n",
    "\n",
    "chat_prompt = ChatPromptTemplate(chat_template)\n",
    "\n",
    "response = llm.complete(chat_prompt.format(question=\"How far did Alexander the Great go in his conquests?\"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander the Great never arrived in China. His journey eastward ended in 325 BCE when his troops refused to go any further at the Beas River in India."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=\"You're a great historian bot.\"),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"When did Alexander the Great arrive in China?\")\n",
    "]\n",
    "\n",
    "response = llm.stream_chat(messages)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’¬ Chat Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: I'm sorry, but I don't understand your request. Could you please provide more context or clarify what you mean by \"flip\"?\n",
      "\n",
      "Assistant: Learning something new can be an exciting and rewarding experience. Here are some steps you can follow to effectively learn something new:\n",
      "\n",
      "1. Identify your interest: Think about what you are passionate about or curious to learn. It could be a new skill, a foreign language, a musical instrument, a sport, or any other area of knowledge. Identifying your interest will help you stay motivated throughout the learning process.\n",
      "\n",
      "2. Set clear goals: Define specific goals for what you want to achieve. Break down your learning journey into smaller, manageable milestones. For example, if you want to learn a new programming language, your goal could be to complete a beginner's course or build a simple project using that language.\n",
      "\n",
      "3. Find resources: Look for resources that align with your learning style and preferences. This could include books, online courses, tutorials, videos, podcasts, or even finding a mentor or joining a community of learners. Choose resources that are reputable and provide structured learning paths.\n",
      "\n",
      "4. Create a learning schedule: Consistency is key when learning something new. Create a study schedule that fits your daily routine and stick to it. Allocate dedicated time slots for learning and minimize distractions during those periods.\n",
      "\n",
      "5. Start with the basics: Begin with the fundamental concepts and build a strong foundation. Don't rush through the initial stages, as a solid understanding of the basics will make it easier to grasp more advanced topics later on.\n",
      "\n",
      "6. Practice actively: Learning is not a passive activity. Engage actively with the material by taking notes, asking questions, and applying what you learn through exercises, projects, or practical applications. The more you practice, the better you'll retain the information.\n",
      "\n",
      "7. Seek feedback and review: Regularly seek feedback on your progress to identify areas for improvement. Review what you've learned at intervals to reinforce your memory and understanding. Spacing out your review sessions can help with long-term retention.\n",
      "\n",
      "8. Connect with others: Join communities, forums, or groups related to your area of interest. Interacting with others who share your passion can provide support, motivation, and new perspectives. Don't be afraid to ask questions or seek help when needed.\n",
      "\n",
      "9. Embrace challenges: Learning something new can be challenging, and it's normal to encounter obstacles. Embrace these challenges as opportunities for growth. Be patient with yourself and maintain a growth mindset, believing that your abilities and intelligence can be developed through effort and practice.\n",
      "\n",
      "10. Celebrate your progress: Recognize and celebrate your achievements along the way. This will boost your motivation and keep you focused on your learning journey. Remember that learning is a lifelong process, and every step forward, no matter how small, is worth acknowledging.\n",
      "\n",
      "Remember, learning is a personal journey, and what works for others may not necessarily work for you. Find what suits your learning style and adapt these steps accordingly. Enjoy the process of discovery and embrace the joy of learning something new!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults(llm=llm)\n",
    "\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_llm',\n",
       " '_memory',\n",
       " '_prefix_messages',\n",
       " 'achat',\n",
       " 'astream_chat',\n",
       " 'callback_manager',\n",
       " 'chat',\n",
       " 'chat_history',\n",
       " 'chat_repl',\n",
       " 'from_defaults',\n",
       " 'reset',\n",
       " 'stream_chat',\n",
       " 'streaming_chat_repl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chat_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: Learning a new technology can be a challenging but rewarding endeavor. Here are some steps to help you get started:\n",
      "\n",
      "1. Define your goal: Start by clearly defining what you want to achieve by learning this new technology. Are you looking to get a new job, build a specific project, or simply expand your knowledge? Having a clear goal will help you stay focused and motivated throughout the learning process.\n",
      "\n",
      "2. Assess your current knowledge: Take an honest look at your current skill set and identify any gaps that you need to fill in order to learn the new technology. This will help you create a more tailored and effective learning plan.\n",
      "\n",
      "3. Find the right resources: There are countless resources available online, including courses, tutorials, books, and videos. Look for resources that are well-reviewed and align with your learning style and goals. It's also helpful to find resources that provide hands-on exercises and projects to reinforce what you're learning.\n",
      "\n",
      "4. Create a structured plan: Break down the technology into smaller, manageable chunks and create a step-by-step plan for learning each aspect. Set realistic milestones and deadlines to help you stay on track.\n",
      "\n",
      "5. Start with the basics: Begin with the fundamental concepts and principles of the technology before diving into more advanced topics. Make sure you have a strong foundation before moving on.\n",
      "\n",
      "6. Practice, practice, practice: Learning a new technology requires hands-on experience. As you learn new concepts, apply them through coding exercises, projects, or experiments. This will help you solidify your understanding and build your skills.\n",
      "\n",
      "7. Join communities and seek help: Engage with online communities, forums, and groups related to the technology you're learning. These communities can provide support, answer questions, and offer additional resources. Don't be afraid to ask for help when you're stuck.\n",
      "\n",
      "8. Stay persistent and patient: Learning a new technology takes time and effort. Be persistent in your studies and patient with yourself. It's normal to encounter challenges and setbacks, but don't let them discourage you. Keep practicing and you will gradually improve.\n",
      "\n",
      "9. Apply your knowledge: Look for opportunities to apply your new skills in real-world scenarios, whether it's through personal projects, open-source contributions, or professional work. This will help you reinforce your learning and build your confidence.\n",
      "\n",
      "10. Continue learning: Technology is constantly evolving, so make sure to stay updated with the latest advancements and trends. Set aside time for continuous learning and exploration to expand your knowledge and stay ahead of the curve.\n",
      "\n",
      "Remember, learning a new technology is a journey, and it's important to enjoy the process. Stay curious, embrace challenges, and celebrate your progress along the way.\n",
      "\n",
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/lil_llama_index/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/conda/envs/lil_llama_index/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 260, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/chat_engine/types.py\", line 164, in write_response_to_history\n",
      "    for chat in self.chat_stream:\n",
      "  File \"/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py\", line 185, in wrapped_gen\n",
      "    for x in f_return_val:\n",
      "  File \"/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/llms/cohere/base.py\", line 212, in gen\n",
      "    for r in response:\n",
      "  File \"/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/cohere/base_client.py\", line 478, in chat_stream\n",
      "    raise ApiError(status_code=_response.status_code, body=_response_json)\n",
      "cohere.core.api_error.ApiError: status_code: 400, body: {'message': 'invalid request: message must be at least 1 token long or tool results must be specified.'}\n"
     ]
    },
    {
     "ename": "ApiError",
     "evalue": "status_code: 400, body: {'message': 'invalid request: message must be at least 1 token long or tool results must be specified.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchat_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreaming_chat_repl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/chat_engine/types.py:352\u001b[0m, in \u001b[0;36mBaseChatEngine.streaming_chat_repl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_chat(message)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 352\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_response_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    354\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/chat_engine/types.py:290\u001b[0m, in \u001b[0;36mStreamingAgentChatResponse.print_response_stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_response_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_gen:\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;28mprint\u001b[39m(token, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/chat_engine/types.py:258\u001b[0m, in \u001b[0;36mStreamingAgentChatResponse.response_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_done \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mempty():\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m         delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mget(block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/threading.py:1016\u001b[0m, in \u001b[0;36mThread._bootstrap_inner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     _sys\u001b[38;5;241m.\u001b[39msetprofile(_profile_hook)\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_excepthook(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/ipykernel/ipkernel.py:766\u001b[0m, in \u001b[0;36mIPythonKernel._initialize_thread_hooks.<locals>.run_closure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m             stream\u001b[38;5;241m.\u001b[39m_thread_to_parent[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mident] \u001b[38;5;241m=\u001b[39m parent\n\u001b[0;32m--> 766\u001b[0m \u001b[43m_threading_Thread_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/threading.py:953\u001b[0m, in \u001b[0;36mThread.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 953\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;66;03m# Avoid a refcycle if the thread is running a function with\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;66;03m# an argument that has a member that points to the thread.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/chat_engine/types.py:164\u001b[0m, in \u001b[0;36mStreamingAgentChatResponse.write_response_to_history\u001b[0;34m(self, memory, on_stream_end_fn)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     final_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_stream:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_function \u001b[38;5;241m=\u001b[39m is_function(chat\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chat\u001b[38;5;241m.\u001b[39mdelta:\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:185\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m last_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f_return_val:\n\u001b[1;32m    186\u001b[0m         dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    187\u001b[0m             LLMChatInProgressEvent(\n\u001b[1;32m    188\u001b[0m                 messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cast(ChatResponse, x)\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/llama_index/llms/cohere/base.py:212\u001b[0m, in \u001b[0;36mCohere.stream_chat.<locals>.gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m role \u001b[38;5;241m=\u001b[39m MessageRole\u001b[38;5;241m.\u001b[39mASSISTANT\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    214\u001b[0m         content_delta \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/opt/conda/envs/lil_llama_index/lib/python3.10/site-packages/cohere/base_client.py:478\u001b[0m, in \u001b[0;36mBaseCohere.chat_stream\u001b[0;34m(self, message, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, citation_quality, temperature, max_tokens, max_input_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, raw_prompting, return_prompt, tools, tool_results, request_options)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mstatus_code, body\u001b[38;5;241m=\u001b[39m_response_json)\n",
      "\u001b[0;31mApiError\u001b[0m: status_code: 400, body: {'message': 'invalid request: message must be at least 1 token long or tool results must be specified.'}"
     ]
    }
   ],
   "source": [
    "chat_engine.streaming_chat_repl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
